# Phase 2: Core Agent Tools - Requirements Planning Document

> **Status:** Planning Document (Pre-Implementation)
> **Created:** 2026-01-13
> **Purpose:** Capture all detailed requirements before creating PHASE_2_HOW_TO_GUIDE.md

**This is a temporary planning document.** Once the how-to guide is created and Phase 2 is complete, this file should be archived or deleted.

---

## Table of Contents

- [Phase 2 Scope Summary](#phase-2-scope-summary)
- [SQL Tool (2b) Requirements](#sql-tool-2b-requirements)
- [RAG Tool (2c) Requirements](#rag-tool-2c-requirements)
- [Knowledge Graph (2c-KG) Requirements](#knowledge-graph-2c-kg-requirements)
- [Data Lifecycle Management](#data-lifecycle-management)
- [Infrastructure Requirements](#infrastructure-requirements)
- [Sample Data Strategy](#sample-data-strategy)
- [Cost Estimates](#cost-estimates)
- [Success Criteria](#success-criteria)
- [Implementation Order](#implementation-order)
- [Open Questions](#open-questions)

---

## Phase 2 Scope Summary

| Tool | Status | Description |
|------|--------|-------------|
| **2a. Tavily Search** | âœ… DONE (Phase 0) | Web search with mock fallback |
| **2b. SQL Query** | ğŸš§ TO IMPLEMENT | Natural language to SQL with Neon PostgreSQL |
| **2c. RAG Document** | ğŸš§ TO IMPLEMENT | Hybrid search with Pinecone + 2026 SOTA techniques |
| **2c-KG. Knowledge Graph** | ğŸš§ TO IMPLEMENT | Neo4j AuraDB with NLP entity extraction |
| **2d. Market Data** | âœ… DONE (Phase 0) | FMP API with mock fallback |

**Focus for Phase 2:** Implementing 2b (SQL), 2c (RAG), and 2c-KG (Knowledge Graph)

---

## SQL Tool (2b) Requirements

### Database Schema Design

The SQL tool uses the existing Neon PostgreSQL database from Phase 1b. We need to create the financial demo schema and seed it with realistic synthetic data.

#### Tables

```sql
-- Customers table
CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    phone VARCHAR(20),
    risk_profile VARCHAR(20) CHECK (risk_profile IN ('conservative', 'moderate', 'aggressive')),
    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'inactive', 'suspended'))
);

-- Accounts table
CREATE TABLE accounts (
    id SERIAL PRIMARY KEY,
    customer_id INTEGER REFERENCES customers(id),
    account_number VARCHAR(20) UNIQUE NOT NULL,
    account_type VARCHAR(20) CHECK (account_type IN ('checking', 'savings', 'investment', 'credit')),
    balance DECIMAL(15, 2) DEFAULT 0.00,
    currency VARCHAR(3) DEFAULT 'USD',
    opened_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'closed', 'frozen'))
);

-- Transactions table
CREATE TABLE transactions (
    id SERIAL PRIMARY KEY,
    account_id INTEGER REFERENCES accounts(id),
    amount DECIMAL(15, 2) NOT NULL,
    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    type VARCHAR(20) CHECK (type IN ('debit', 'credit', 'transfer')),
    description VARCHAR(255),
    category VARCHAR(50),
    reference_number VARCHAR(50)
);

-- Portfolios table
CREATE TABLE portfolios (
    id SERIAL PRIMARY KEY,
    customer_id INTEGER REFERENCES customers(id),
    name VARCHAR(100) NOT NULL,
    total_value DECIMAL(15, 2) DEFAULT 0.00,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    risk_level VARCHAR(20) CHECK (risk_level IN ('low', 'medium', 'high'))
);

-- Trades table
CREATE TABLE trades (
    id SERIAL PRIMARY KEY,
    portfolio_id INTEGER REFERENCES portfolios(id),
    symbol VARCHAR(10) NOT NULL,
    quantity INTEGER NOT NULL,
    price DECIMAL(10, 4) NOT NULL,
    trade_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    trade_type VARCHAR(10) CHECK (trade_type IN ('buy', 'sell')),
    status VARCHAR(20) DEFAULT 'completed' CHECK (status IN ('pending', 'completed', 'cancelled'))
);

-- Indexes for common queries
CREATE INDEX idx_transactions_account_id ON transactions(account_id);
CREATE INDEX idx_transactions_date ON transactions(transaction_date);
CREATE INDEX idx_accounts_customer_id ON accounts(customer_id);
CREATE INDEX idx_trades_portfolio_id ON trades(portfolio_id);
CREATE INDEX idx_trades_symbol ON trades(symbol);
```

### Sample Data Volume

| Table | Row Count | Generation Strategy |
|-------|-----------|---------------------|
| customers | ~500 | Faker names, emails, varied risk profiles |
| accounts | ~1,500 | 3 accounts per customer average |
| transactions | ~10,000 | Random transactions over 12 months |
| portfolios | ~200 | Investment customers only |
| trades | ~5,000 | Stock trades (AAPL, MSFT, GOOGL, AMZN, etc.) |

### SQL Tool Implementation

#### ALLOWED_TABLES Whitelist

```python
ALLOWED_TABLES = {
    "customers",
    "accounts", 
    "transactions",
    "portfolios",
    "trades"
}

ALLOWED_COLUMNS = {
    "customers": ["id", "name", "email", "phone", "risk_profile", "created_date", "status"],
    "accounts": ["id", "customer_id", "account_number", "account_type", "balance", "currency", "opened_date", "status"],
    "transactions": ["id", "account_id", "amount", "transaction_date", "type", "description", "category", "reference_number"],
    "portfolios": ["id", "customer_id", "name", "total_value", "last_updated", "risk_level"],
    "trades": ["id", "portfolio_id", "symbol", "quantity", "price", "trade_date", "trade_type", "status"]
}
```

#### Natural Language to SQL Prompt

```
You are a SQL query generator for a financial database.

Available tables and columns:
- customers: id, name, email, phone, risk_profile, created_date, status
- accounts: id, customer_id, account_number, account_type, balance, currency, opened_date, status
- transactions: id, account_id, amount, transaction_date, type, description, category, reference_number
- portfolios: id, customer_id, name, total_value, last_updated, risk_level
- trades: id, portfolio_id, symbol, quantity, price, trade_date, trade_type, status

Rules:
1. Only use SELECT statements (no INSERT, UPDATE, DELETE, DROP, etc.)
2. Only query the tables listed above
3. Use proper JOINs when relating tables
4. Limit results to 100 rows maximum
5. Use parameterized placeholders (:param_name) for any user-provided values

User query: {user_query}

Generate a safe, read-only SQL query:
```

#### Safety Checks

1. **Query validation:** Parse and validate SQL before execution
2. **Read-only enforcement:** Only allow SELECT statements
3. **Table whitelisting:** Reject queries with unlisted tables
4. **Result limits:** Cap at 1000 rows, default 100
5. **Timeout:** 30 second query timeout
6. **Parameterization:** All user values must be parameterized

### Files to Create

| File | Purpose |
|------|---------|
| `backend/alembic/versions/002_financial_schema.py` | Schema migration |
| `backend/alembic/versions/003_seed_data.py` | Seed data migration with Faker |
| `backend/src/agent/tools/sql.py` | SQL tool implementation (upgrade stub) |
| `backend/src/agent/tools/sql_safety.py` | ALLOWED_TABLES, query validation |

### Sample Queries the Agent Should Handle

- "What's the total balance for customer John Doe?"
- "Show me all transactions over $1000 last month"
- "Which customers have investment accounts?"
- "What's the portfolio value for customer ID 123?"
- "Show me all trades for AAPL in the last 30 days"
- "List the top 10 customers by total account balance"
- "How many transactions happened yesterday?"
- "What's the average trade size for MSFT?"

---

## RAG Tool (2c) Requirements

### 2026 State-of-the-Art Architecture

The RAG implementation uses modern techniques for maximum retrieval quality:

| Technique | Impact | Cost |
|-----------|--------|------|
| Semantic Chunking | +10-15% relevance | $0 (ingestion time) |
| Contextual Retrieval | +15-20% precision | $0 (ingestion time) |
| Hybrid Search (Dense + BM25) | +20-30% recall | $0 |
| Query Expansion | +20-30% recall | ~$0.005/query |
| Cross-Encoder Reranking | +20-25% precision | ~$0.015/query |
| Knowledge Graph | +15-25% precision | ~$0.01/query |
| **Total Query Cost** | | **~$0.03-0.04/query** |

### Ingestion Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        S3 Document Upload                        â”‚
â”‚                    (PDF, TXT, MD, HTML)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ S3 Event Notification
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lambda: Document Ingestion                    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Document Processor                      â”‚   â”‚
â”‚  â”‚  â€¢ PDF parsing (PyPDF2 or pdfplumber)                    â”‚   â”‚
â”‚  â”‚  â€¢ Text extraction                                        â”‚   â”‚
â”‚  â”‚  â€¢ Metadata extraction (title, author, date, type)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                   Semantic Chunking                       â”‚   â”‚
â”‚  â”‚  â€¢ spaCy sentence boundary detection                     â”‚   â”‚
â”‚  â”‚  â€¢ Paragraph-aware splitting                              â”‚   â”‚
â”‚  â”‚  â€¢ Max chunk size: 512 tokens                            â”‚   â”‚
â”‚  â”‚  â€¢ Overlap: 50 tokens                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                Contextual Enrichment                      â”‚   â”‚
â”‚  â”‚  â€¢ Prepend document title                                 â”‚   â”‚
â”‚  â”‚  â€¢ Add section header                                     â”‚   â”‚
â”‚  â”‚  â€¢ Include document type                                  â”‚   â”‚
â”‚  â”‚  Format: "[Title: X] [Section: Y] [Type: Z] {chunk}"     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚          â–¼                 â–¼                 â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Dense Embed â”‚   â”‚ Sparse/BM25 â”‚   â”‚ NLP Entity  â”‚           â”‚
â”‚  â”‚ (Titan)     â”‚   â”‚ Index       â”‚   â”‚ Extraction  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚                 â”‚                 â”‚                   â”‚
â”‚         â–¼                 â–¼                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Pinecone   â”‚   â”‚  Pinecone   â”‚   â”‚ Neo4j       â”‚           â”‚
â”‚  â”‚  (vectors)  â”‚   â”‚  (sparse)   â”‚   â”‚ AuraDB      â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Query                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Query Expansion                               â”‚
â”‚            (Generate 3 alternative phrasings)                    â”‚
â”‚                     via Nova Lite                                â”‚
â”‚                                                                  â”‚
â”‚  Original: "What is the refund policy?"                         â”‚
â”‚  Variant 1: "How can I get a refund?"                           â”‚
â”‚  Variant 2: "What are the terms for returning a product?"       â”‚
â”‚  Variant 3: "Refund and return procedures"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense Searchâ”‚     â”‚ BM25 Search â”‚     â”‚  KG Lookup  â”‚
â”‚ (Pinecone)  â”‚     â”‚ (Pinecone)  â”‚     â”‚ (Neo4j)     â”‚
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â”‚ top_k=15    â”‚     â”‚ top_k=15    â”‚     â”‚ 1-2 hops    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RRF Fusion                                    â”‚
â”‚         (Reciprocal Rank Fusion - merge all results)            â”‚
â”‚                                                                  â”‚
â”‚  Score = Î£ 1/(k + rank) for each result across all sources      â”‚
â”‚  k = 60 (standard constant)                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Cross-Encoder Reranking                          â”‚
â”‚              (LLM scores top 15 for relevance)                   â”‚
â”‚                                                                  â”‚
â”‚  Prompt: "Rate relevance 1-10: Query: {q}, Document: {d}"       â”‚
â”‚  Return: top 5 highest scoring                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Contextual Compression                            â”‚
â”‚           (Extract only relevant portions)                       â”‚
â”‚                                                                  â”‚
â”‚  LLMChainExtractor: Keep sentences relevant to query            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Final Results                                 â”‚
â”‚                                                                  â”‚
â”‚  â€¢ Relevant text chunks                                         â”‚
â”‚  â€¢ Source citations (document, page, section)                   â”‚
â”‚  â€¢ Confidence scores                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pinecone Index Configuration

```python
# Index specification
index_name = "enterprise-agentic-ai"
dimension = 1536  # Titan embeddings
metric = "cosine"
cloud = "aws"
region = "us-east-1"

# Metadata fields to store
metadata_fields = [
    "document_id",      # Unique document identifier
    "document_title",   # Document title
    "document_type",    # policy, regulation, faq, product_doc
    "section",          # Section header if available
    "page_number",      # Page number for PDFs
    "chunk_index",      # Position within document
    "created_date",     # Document creation date
    "source_url",       # S3 URL for source
]

# Sparse vector for hybrid search
# Uses Pinecone's built-in sparse-dense hybrid
```

### Files to Create

| File | Purpose |
|------|---------|
| `backend/src/ingestion/document_processor.py` | Main document processing orchestrator |
| `backend/src/ingestion/semantic_chunking.py` | spaCy-based semantic chunking |
| `backend/src/ingestion/contextual_chunking.py` | Context prepending for chunks |
| `backend/src/ingestion/chunking.py` | Parent document retriever pattern |
| `backend/src/ingestion/query_expansion.py` | Query expansion via LLM |
| `backend/src/utils/embeddings.py` | Bedrock Titan embedding wrapper |
| `backend/src/utils/rrf.py` | Reciprocal Rank Fusion implementation |
| `backend/src/utils/reranker.py` | Cross-encoder reranking |
| `backend/src/agent/tools/rag.py` | RAG tool (upgrade from stub) |
| `lambda/document-ingestion/handler.py` | S3 trigger Lambda handler |
| `lambda/document-ingestion/requirements.txt` | Lambda dependencies |

---

## Knowledge Graph (2c-KG) Requirements

### Neo4j AuraDB Setup

| Attribute | Value |
|-----------|-------|
| Service | Neo4j AuraDB Free |
| Node Limit | 200,000 |
| Relationship Limit | 400,000 |
| Cost | $0/month |
| Region | Should match us-east-1 if available |

### Local Development

```yaml
# docker-compose.yml addition
services:
  neo4j:
    image: neo4j:5-community
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/localdevpassword
      - NEO4J_PLUGINS=["apoc"]
    volumes:
      - neo4j_data:/data
```

### Financial Domain Ontology

#### Entity Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ENTITY TYPES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Document â”€â”€â”€â”€â”€â”€â”€ Source document in RAG system                 â”‚
â”‚     â”‚              (PDF, policy, FAQ, etc.)                     â”‚
â”‚     â”‚                                                            â”‚
â”‚  Policy â”€â”€â”€â”€â”€â”€â”€â”€â”€  Company policy or procedure                  â”‚
â”‚     â”‚              (refund policy, privacy policy)              â”‚
â”‚     â”‚                                                            â”‚
â”‚  Regulation â”€â”€â”€â”€â”€  External regulation or law                   â”‚
â”‚     â”‚              (SEC, FINRA, GDPR)                           â”‚
â”‚     â”‚                                                            â”‚
â”‚  Concept â”€â”€â”€â”€â”€â”€â”€â”€  Financial concept or term                    â”‚
â”‚     â”‚              (APR, compound interest, ETF)                â”‚
â”‚     â”‚                                                            â”‚
â”‚  Product â”€â”€â”€â”€â”€â”€â”€â”€  Financial product                            â”‚
â”‚     â”‚              (checking account, credit card)              â”‚
â”‚     â”‚                                                            â”‚
â”‚  Person â”€â”€â”€â”€â”€â”€â”€â”€â”€  Named person mentioned                       â”‚
â”‚     â”‚              (executives, contacts)                        â”‚
â”‚     â”‚                                                            â”‚
â”‚  Organization â”€â”€â”€  Company or institution                       â”‚
â”‚                    (partner banks, regulators)                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Relationship Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RELATIONSHIP TYPES                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  MENTIONS â”€â”€â”€â”€â”€â”€â”€â”€  Document mentions an entity                 â”‚
â”‚                     (Document)-[:MENTIONS]->(Entity)            â”‚
â”‚                                                                  â”‚
â”‚  DEFINES â”€â”€â”€â”€â”€â”€â”€â”€â”€  Document defines a concept                  â”‚
â”‚                     (Document)-[:DEFINES]->(Concept)            â”‚
â”‚                                                                  â”‚
â”‚  GOVERNED_BY â”€â”€â”€â”€â”€  Entity governed by regulation               â”‚
â”‚                     (Product)-[:GOVERNED_BY]->(Regulation)      â”‚
â”‚                                                                  â”‚
â”‚  APPLIES_TO â”€â”€â”€â”€â”€â”€  Policy applies to product/customer          â”‚
â”‚                     (Policy)-[:APPLIES_TO]->(Product)           â”‚
â”‚                                                                  â”‚
â”‚  RELATED_TO â”€â”€â”€â”€â”€â”€  Generic relationship between entities       â”‚
â”‚                     (Entity)-[:RELATED_TO]->(Entity)            â”‚
â”‚                                                                  â”‚
â”‚  SIMILAR_TO â”€â”€â”€â”€â”€â”€  Concept similarity                          â”‚
â”‚                     (Concept)-[:SIMILAR_TO]->(Concept)          â”‚
â”‚                                                                  â”‚
â”‚  SUPERSEDES â”€â”€â”€â”€â”€â”€  Newer policy replaces older                 â”‚
â”‚                     (Policy)-[:SUPERSEDES]->(Policy)            â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### NLP Entity Extraction (spaCy-based)

Using spaCy instead of LLM for cost efficiency:

```python
# spaCy NER entity types to extract
SPACY_ENTITY_TYPES = [
    "PERSON",      # People, including fictional
    "ORG",         # Companies, agencies, institutions
    "GPE",         # Countries, cities, states
    "DATE",        # Dates or periods
    "MONEY",       # Monetary values
    "PERCENT",     # Percentages
    "LAW",         # Named documents made into laws
    "PRODUCT",     # Objects, vehicles, foods, etc.
]

# Custom patterns for financial domain
FINANCIAL_PATTERNS = [
    {"label": "REGULATION", "pattern": [{"TEXT": {"REGEX": r"(SEC|FINRA|FDIC|OCC|CFPB)"}}]},
    {"label": "PRODUCT", "pattern": [{"LOWER": {"IN": ["checking", "savings", "credit", "debit"]}}, {"LOWER": {"IN": ["account", "card"]}}]},
    {"label": "CONCEPT", "pattern": [{"LOWER": {"IN": ["apr", "apy", "interest", "fee", "balance"]}}]},
]
```

### Cost Comparison

| Method | Cost per Document | 1000 Documents |
|--------|-------------------|----------------|
| LLM-based extraction | $0.02-0.05 | $20-50 |
| spaCy NLP extraction | ~$0.001 | ~$1 |
| **Savings** | **95-98%** | **$19-49** |

### Files to Create

| File | Purpose |
|------|---------|
| `backend/src/knowledge_graph/__init__.py` | Package exports |
| `backend/src/knowledge_graph/efficient_extractor.py` | spaCy NER + custom patterns |
| `backend/src/knowledge_graph/store.py` | Neo4j connection and CRUD |
| `backend/src/knowledge_graph/ontology.py` | Entity and relationship types |
| `backend/src/knowledge_graph/queries.py` | Graph traversal queries |

---

## Data Lifecycle Management

### SQL Data Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SQL DATA LIFECYCLE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Initial Setup:                                                  â”‚
â”‚  1. Run Alembic migration: 002_financial_schema.py              â”‚
â”‚  2. Run Alembic migration: 003_seed_data.py                     â”‚
â”‚  3. Verify: SELECT COUNT(*) FROM each table                     â”‚
â”‚                                                                  â”‚
â”‚  Data Updates:                                                   â”‚
â”‚  â€¢ Manual via new Alembic migrations                            â”‚
â”‚  â€¢ No auto-update mechanism needed for demo                     â”‚
â”‚                                                                  â”‚
â”‚  Reset/Refresh:                                                  â”‚
â”‚  1. alembic downgrade base                                       â”‚
â”‚  2. alembic upgrade head                                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG Document Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG DOCUMENT LIFECYCLE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Document Upload:                                                â”‚
â”‚  1. Upload to S3 bucket: s3://enterprise-agentic-ai-docs/       â”‚
â”‚  2. Lambda automatically triggered                               â”‚
â”‚  3. Document processed, chunked, embedded                       â”‚
â”‚  4. Vectors stored in Pinecone                                  â”‚
â”‚  5. Entities extracted and stored in Neo4j                      â”‚
â”‚                                                                  â”‚
â”‚  Supported Formats:                                              â”‚
â”‚  â€¢ PDF (.pdf) - parsed with PyPDF2/pdfplumber                   â”‚
â”‚  â€¢ Text (.txt) - direct text processing                         â”‚
â”‚  â€¢ Markdown (.md) - rendered then processed                     â”‚
â”‚  â€¢ HTML (.html) - BeautifulSoup extraction                      â”‚
â”‚                                                                  â”‚
â”‚  Document Update:                                                â”‚
â”‚  1. Upload new version with same name (overwrites)              â”‚
â”‚  2. Lambda deletes old vectors by document_id                   â”‚
â”‚  3. Re-processes and re-indexes                                 â”‚
â”‚                                                                  â”‚
â”‚  Document Delete:                                                â”‚
â”‚  1. Delete from S3                                               â”‚
â”‚  2. Manual cleanup: delete vectors by document_id               â”‚
â”‚  3. Manual cleanup: delete KG nodes by document_id              â”‚
â”‚                                                                  â”‚
â”‚  Metadata Storage (Pinecone):                                   â”‚
â”‚  â€¢ document_id, title, type, section, page, chunk_index         â”‚
â”‚  â€¢ Enables filtering: type="policy", date > "2024-01-01"        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Knowledge Graph Management

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               KNOWLEDGE GRAPH LIFECYCLE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Auto-Population:                                                â”‚
â”‚  â€¢ Entities extracted during document ingestion                 â”‚
â”‚  â€¢ Relationships inferred from co-occurrence                    â”‚
â”‚  â€¢ No manual maintenance required                               â”‚
â”‚                                                                  â”‚
â”‚  Entity Deduplication:                                          â”‚
â”‚  â€¢ Fuzzy matching on entity names (Levenshtein distance)        â”‚
â”‚  â€¢ Merge similar entities (e.g., "SEC" and "S.E.C.")           â”‚
â”‚  â€¢ Canonical name resolution                                    â”‚
â”‚                                                                  â”‚
â”‚  Relationship Inference:                                        â”‚
â”‚  â€¢ Co-occurrence: entities in same chunk = RELATED_TO          â”‚
â”‚  â€¢ Pattern-based: "governed by" â†’ GOVERNED_BY                   â”‚
â”‚  â€¢ Temporal: newer policy â†’ SUPERSEDES older                    â”‚
â”‚                                                                  â”‚
â”‚  Graph Queries:                                                  â”‚
â”‚  â€¢ 1-hop: Find all documents mentioning entity X                â”‚
â”‚  â€¢ 2-hop: Find entities related to entities in query            â”‚
â”‚  â€¢ Path: Find connection between two entities                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Infrastructure Requirements

### New AWS Resources

| Resource | Purpose | Cost Estimate |
|----------|---------|---------------|
| S3 Bucket | Document storage | ~$0.50/month |
| Lambda Function | Document ingestion | ~$0.10/month |
| IAM Policies | Lambda permissions | $0 |

### Terraform Modules to Create

```
terraform/modules/
â”œâ”€â”€ documents-s3/
â”‚   â”œâ”€â”€ main.tf      # S3 bucket with Lambda trigger
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â””â”€â”€ document-lambda/
    â”œâ”€â”€ main.tf      # Lambda function
    â”œâ”€â”€ variables.tf
    â””â”€â”€ outputs.tf
```

### External Services Setup

#### Pinecone (Free Tier)

1. Create account at https://pinecone.io
2. Create serverless index:
   - Name: `enterprise-agentic-ai`
   - Dimensions: 1536
   - Metric: cosine
   - Cloud: AWS
   - Region: us-east-1
3. Copy API key to `.env` and Secrets Manager

#### Neo4j AuraDB (Free Tier)

1. Create account at https://neo4j.com/cloud/aura-free/
2. Create free instance:
   - Name: `enterprise-agentic-ai`
   - Region: Closest to us-east-1
3. Note connection URI, username, password
4. Add to `.env` and Secrets Manager

### New Environment Variables

```bash
# Add to .env.example and Secrets Manager

# Pinecone (Vector Store)
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=enterprise-agentic-ai
PINECONE_ENVIRONMENT=us-east-1  # or gcp-starter for free

# Neo4j (Knowledge Graph)
NEO4J_URI=neo4j+s://xxxxx.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# Document Storage
DOCUMENTS_BUCKET_NAME=enterprise-agentic-ai-documents-xxxxx
```

---

## Sample Data Strategy

### SQL Sample Data (Faker-generated)

```python
# Seed data generation approach
from faker import Faker
import random

fake = Faker()

# Customers: 500 with realistic names, emails
customers = [
    {
        "name": fake.name(),
        "email": fake.email(),
        "phone": fake.phone_number(),
        "risk_profile": random.choice(["conservative", "moderate", "aggressive"]),
        "status": random.choice(["active"] * 9 + ["inactive"])  # 90% active
    }
    for _ in range(500)
]

# Accounts: ~3 per customer
# Transactions: ~20 per account over 12 months
# Portfolios: Only for aggressive/moderate customers
# Trades: Random stock trades (AAPL, MSFT, GOOGL, AMZN, TSLA, META, NVDA)
```

### RAG Sample Documents

| Category | Documents | Description |
|----------|-----------|-------------|
| Company Policies | 5 | Refund, privacy, terms, security, compliance |
| Financial Regulations | 3 | SEC guidelines, FINRA rules, banking regs |
| Product Documentation | 5 | Account types, credit cards, investment products |
| FAQs | 3 | Common questions, troubleshooting, how-tos |
| **Total** | **16** | |

Sample document titles:
- `refund-policy.pdf` - Company refund and return policy
- `privacy-policy.pdf` - Data privacy and protection
- `terms-of-service.pdf` - Terms and conditions
- `sec-regulation-summary.pdf` - SEC compliance overview
- `checking-account-guide.pdf` - Checking account features
- `investment-products.pdf` - Investment options guide
- `credit-card-terms.pdf` - Credit card terms and fees
- `faq-account-management.pdf` - Account FAQ
- `faq-transactions.pdf` - Transaction FAQ
- `security-guidelines.pdf` - Security best practices

---

## Cost Estimates

### Phase 2 Additional Monthly Costs

| Component | Monthly Cost | Notes |
|-----------|--------------|-------|
| Pinecone Free | $0 | 100K vectors included |
| Neo4j AuraDB Free | $0 | 200K nodes included |
| S3 (documents) | ~$0.50 | <1GB storage |
| Lambda invocations | ~$0.10 | ~1000 invocations |
| Bedrock embeddings | ~$1-3 | ~$0.0001/1K tokens |
| Bedrock reranking | ~$0.50-1 | ~$0.015/query Ã— queries |
| **Total Phase 2 Addition** | **~$2-5** | |

### Running Total

| Phase | Monthly Cost |
|-------|--------------|
| Phase 1b (current) | ~$10-25 |
| Phase 2 additions | ~$2-5 |
| **Total** | **~$12-30** |

Still well under the $50/month target!

---

## Success Criteria

### SQL Tool (2b)

- [ ] Schema deployed to Neon PostgreSQL
- [ ] Seed data populated (~500 customers, ~10K transactions)
- [ ] Natural language queries return correct SQL
- [ ] ALLOWED_TABLES whitelist enforced
- [ ] Parameterized queries only (no SQL injection)
- [ ] Query timeout and result limits work
- [ ] Error messages are user-friendly

### RAG Tool (2c)

- [ ] Documents can be uploaded to S3
- [ ] Lambda processes documents automatically
- [ ] Chunks stored in Pinecone with metadata
- [ ] Hybrid search (dense + sparse) works
- [ ] Query expansion generates 3 variants
- [ ] Cross-encoder reranking improves relevance
- [ ] Source citations included in results
- [ ] Fallback works when Pinecone unavailable

### Knowledge Graph (2c-KG)

- [ ] Neo4j AuraDB connected and working
- [ ] spaCy extracts entities from documents
- [ ] Entities stored in graph with relationships
- [ ] 1-hop queries return relevant documents
- [ ] 2-hop traversal finds related entities
- [ ] Graph enhances RAG retrieval quality

### Integration

- [ ] All tools registered in LangGraph agent
- [ ] Agent selects appropriate tool for query
- [ ] Multi-tool queries work (e.g., SQL + RAG)
- [ ] Streaming responses include tool usage
- [ ] Error recovery handles tool failures gracefully

### Quality Metrics

- [ ] RAGAS Faithfulness > 0.7
- [ ] RAGAS Answer Relevancy > 0.7
- [ ] RAGAS Context Precision > 0.7
- [ ] RAGAS Context Recall > 0.7

---

## Implementation Order

### Phase 2a: SQL Tool (Week 1)

1. Create Alembic migration for schema
2. Create Alembic migration for seed data
3. Implement SQL tool with safety checks
4. Test natural language to SQL queries
5. Integrate with LangGraph agent

### Phase 2b: Basic RAG (Week 2)

1. Set up Pinecone index
2. Implement document processor
3. Implement semantic chunking
4. Implement contextual enrichment
5. Create basic RAG tool (dense search only)

### Phase 2c: Document Ingestion (Week 2-3)

1. Create S3 bucket with Terraform
2. Create Lambda function
3. Connect S3 trigger to Lambda
4. Test document upload â†’ indexing flow

### Phase 2d: Knowledge Graph (Week 3)

1. Set up Neo4j AuraDB
2. Implement spaCy entity extractor
3. Implement graph store adapter
4. Connect to ingestion pipeline

### Phase 2e: Advanced RAG (Week 3-4)

1. Add BM25 sparse vectors
2. Implement RRF fusion
3. Add query expansion
4. Implement cross-encoder reranking
5. Add contextual compression

### Phase 2f: Integration & Testing (Week 4)

1. Register all tools in agent
2. End-to-end testing
3. RAGAS evaluation
4. Documentation and cleanup

---

## Open Questions

> These questions should be resolved before or during implementation.

### SQL Tool

1. **Read-only user:** Should we create a separate Neon user with SELECT-only permissions, or use query validation?
   - **Recommendation:** Query validation is simpler; create read-only user only if time permits

2. **Query explanation:** Should the agent explain the SQL it generates to users?
   - **Recommendation:** Yes, include in tool response for transparency

### RAG Tool

3. **Chunk size:** 512 tokens with 50 overlap, or different values?
   - **Recommendation:** Start with 512/50, tune based on RAGAS metrics

4. **Reranking model:** Use Nova Lite or a dedicated cross-encoder model?
   - **Recommendation:** Nova Lite for simplicity; switch to dedicated model if quality insufficient

5. **Parent document retriever:** Implement full pattern or simplified version?
   - **Recommendation:** Simplified first (contextual enrichment covers most benefit)

### Knowledge Graph

6. **Neo4j vs PostgreSQL:** Final decision on graph store?
   - **Decision:** Neo4j AuraDB Free (as per user choice)

7. **Entity deduplication:** How aggressive should fuzzy matching be?
   - **Recommendation:** Levenshtein distance â‰¤ 2 for matching

### Infrastructure

8. **Lambda timeout:** 15 seconds default or longer for large PDFs?
   - **Recommendation:** 60 seconds to handle larger documents

9. **Lambda memory:** 256MB default or more for spaCy?
   - **Recommendation:** 512MB-1GB for spaCy model loading

---

## References

- [PROJECT_PLAN.md](../PROJECT_PLAN.md) - Phase 2 section
- [DEVELOPMENT_REFERENCE.md](../DEVELOPMENT_REFERENCE.md) - Phase 2 specs
- [backend.mdc](../.cursor/rules/backend.mdc) - Python patterns
- [agent.mdc](../.cursor/rules/agent.mdc) - LangGraph patterns
- [_security.mdc](../.cursor/rules/_security.mdc) - SQL safety patterns
- [infrastructure.mdc](../.cursor/rules/infrastructure.mdc) - Terraform patterns

---

*This document will be used as the basis for PHASE_2_HOW_TO_GUIDE.md*

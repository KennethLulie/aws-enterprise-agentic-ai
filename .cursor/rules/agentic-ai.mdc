---
description: Agentic AI patterns - LangGraph orchestration, tool calling, multi-tool coordination, state management
globs:
  - backend/src/agent/**
alwaysApply: false
---

# Agentic AI Development Rules

## LangGraph Patterns

**State Schema:**
- Use TypedDict for state (LangGraph requirement, not Pydantic)
- Define in `backend/src/agent/state.py`
- Required fields: messages, conversation_id, tools_used, last_error, metadata
- Example:
```python
from typing import TypedDict, List, Optional, Dict, Any
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    messages: List[BaseMessage]
    conversation_id: Optional[str]
    tools_used: List[str]
    last_error: Optional[str]
    metadata: Dict[str, Any]
```

**Node Functions:**
- Signature: `async def node_name(state: AgentState) -> AgentState:`
- Always return updated state (don't mutate in place)
- Use type hints from `src.agent.state`
- Keep nodes focused: One responsibility per node
- Example:
```python
from src.agent.state import AgentState

async def chat_node(state: AgentState) -> AgentState:
    """Process user message with LLM."""
    # Implementation
    return state
```

**Graph Definition:**
- Use `StateGraph` from langgraph
- Define nodes: `graph.add_node("node_name", node_function)`
- Define edges: `graph.add_edge("node1", "node2")`
- Conditional edges: `graph.add_conditional_edges("node", condition_func, path_map)`
- Set entry point: `graph.set_entry_point("first_node")`
- Compile with checkpointing: `graph.compile(checkpointer=checkpointer)`
- Reference: `backend/src/agent/graph.py`

**Checkpointing:**
- Phase 0: MemorySaver (in-memory, no DB dependency, faster for dev)
- Phase 1b+: PostgresSaver (persistent state, requires Aurora)
- Always use checkpointing for conversation state persistence
- Checkpoint key: conversation_id

**Streaming:**
- Use `agent.astream()` for async streaming
- Stream both LLM tokens and tool execution events
- Event types: "messages", "tool_calls", "tool_results", "errors"
- Yield events as they occur (don't batch)

## Tool Orchestration

**Tool Binding:**
- Bind tools to LLM using LangChain tool binding
- Tools must be LangChain-compatible (use `@tool` decorator or StructuredTool)
- Tool execution happens in separate node (not in chat node)
- Reference: `backend/src/agent/tools/` directory

**Tool Definition Pattern:**
```python
from langchain.tools import tool
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    query: str = Field(..., description="Search query")

@tool(args_schema=SearchInput)
async def search_tool(query: str) -> str:
    """Search the web for information."""
    # Implementation
    return results
```

**Tool Selection:**
- LLM decides which tools to call (don't hardcode tool selection)
- Provide clear tool descriptions for LLM
- Limit tool set to prevent confusion (4-5 tools max for demo)
- Tools: Search (Tavily), SQL (Aurora), RAG (Pinecone), Weather (OpenWeatherMap)

**Tool Execution Node:**
- Separate node for tool execution (not in chat node)
- Execute tools based on LLM tool calls
- Handle tool errors gracefully
- Return tool results to state
- Reference: `backend/src/agent/nodes/tools.py`

**Multi-Tool Coordination:**
- Agent can call multiple tools in sequence or parallel
- Use tool results to inform next tool calls
- Example flow: Search → SQL Query → RAG Retrieval → Final Answer
- Track tools_used in state for observability

## Multi-Tool Use Cases

**Business Scenario Patterns:**

**1. Research + Data Analysis:**
- User asks: "What are the latest trends in AI and how do they compare to our sales data?"
- Flow: Search (trends) → SQL Query (sales data) → Synthesize answer
- Demonstrates: External info + Internal data combination

**2. Customer Support:**
- User asks: "What's our return policy for product X and current inventory?"
- Flow: RAG (policy docs) → SQL Query (inventory) → Answer
- Demonstrates: Document retrieval + Database query

**3. Market Analysis:**
- User asks: "What's the weather in Austin and how does it affect our sales?"
- Flow: Weather API → SQL Query (sales by location/date) → Analysis
- Demonstrates: External API + Database correlation

**Implementation Pattern:**
- Design use cases that require 2+ tools
- Make tool coordination visible in UI (show which tools were used)
- Log multi-tool flows for demo presentation
- Ensure smooth execution (no unnecessary delays)

## Error Recovery

**Error Recovery Node:**
- Handle errors gracefully without crashing agent
- Retry failed operations when appropriate
- Fallback to simpler operations if complex ones fail
- Return user-friendly error messages
- Reference: `backend/src/agent/nodes/error_recovery.py`

**Error Recovery Patterns:**
- Tool failure: Try alternative tool or return partial answer
- LLM failure: Fallback to Claude if Nova fails
- Database failure: Return cached data if available
- Rate limit: Queue request or return helpful message

**Circuit Breakers:**
- Implement circuit breakers for external services
- Open circuit after N failures
- Half-open after timeout to test recovery
- Close circuit when service recovers
- Use tenacity for retry logic

**Fallback Strategies:**
- Primary model fails → Fallback model
- Tool fails → Return partial answer or error message
- Database fails → Use cached data or return error
- All tools fail → Return LLM-only answer (no tools)

## Streaming Patterns

**Thought Process Visualization:**
- Stream agent thinking process (if implemented)
- Show tool selection reasoning
- Display tool execution status
- Update UI progressively as agent works

**Streaming Event Types:**
- `message`: LLM token chunks
- `tool_call`: Agent decided to use a tool
- `tool_result`: Tool execution completed
- `error`: Error occurred (user-friendly message)
- `complete`: Agent finished processing

**Frontend Integration:**
- Parse SSE events by type
- Update UI based on event type
- Show tool execution indicators
- Display progressive text rendering
- Reference: frontend.mdc for SSE patterns

## Cost Optimization

**Model Selection:**
- Use Nova Pro for main chat (best quality, reasonable cost)
- Use Nova Lite for verification/guards (cheaper, sufficient)
- Fallback to Claude only if Nova unavailable (more expensive)
- Monitor token usage per request

**Token Management:**
- Limit conversation history length (prevent token bloat)
- Summarize old messages if conversation gets long
- Use streaming to reduce perceived latency
- Cache embeddings for RAG (don't regenerate)

**Tool Usage Optimization:**
- Only call tools when necessary (let LLM decide)
- Cache tool results when possible
- Batch tool calls when appropriate
- Use cheaper tools first (e.g., RAG before Search)

**Caching (Phase 4+):**
- Cache LLM responses in DynamoDB
- Use semantic similarity matching (threshold: 0.95)
- TTL: 7 days default
- Cache key: embedding of user message + conversation context

## State Management

**State Updates:**
- Always return new state (don't mutate in place)
- Use state spread: `{**state, "messages": new_messages}`
- Keep state immutable where possible
- Track state changes for debugging

**State Persistence:**
- Use checkpointing for conversation persistence
- Checkpoint after each node execution
- Load state from checkpoint on conversation resume
- Clear state on conversation end (optional)

**State Validation:**
- Validate state structure before processing
- Ensure required fields are present
- Handle missing fields gracefully
- Log state validation errors

## Testing Agentic Systems

**Mocking LLM Responses:**
- Mock Bedrock client responses
- Test different response scenarios (tool calls, errors, etc.)
- Use pytest fixtures for common mocks
- Test fallback behavior

**Testing Tool Calls:**
- Mock tool execution
- Test tool selection logic
- Test tool error handling
- Test multi-tool coordination

**Testing State Transitions:**
- Test state updates in each node
- Verify state structure after transitions
- Test error recovery state updates
- Test checkpointing/loading

**Integration Testing:**
- Test full agent flow end-to-end
- Test streaming behavior
- Test error recovery paths
- Test multi-tool scenarios

## Observability

**Logging:**
- Log all tool calls with context
- Log state transitions
- Log errors with full context
- Use structured logging (structlog)

**Metrics:**
- Track tool usage frequency
- Track LLM token usage
- Track request latency
- Track error rates

**Tracing (Phase 5+):**
- Use Arize Phoenix for tracing
- Trace LLM calls, tool execution, state transitions
- Track costs per request
- Analyze agent behavior patterns
